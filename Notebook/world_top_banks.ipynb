{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ETL Pipeline: World’s Largest Banks Data Processing\n",
        "### IBM Data Engineering Specialization – Portfolio Project\n",
        "Objective: Build an ETL pipeline to extract top-10 banks by market cap (USD), convert to GBP/EUR/INR using exchange_rate.csv, save results to CSV and DB, and maintain a code log.\n"
      ],
      "metadata": {
        "id": "lFaEOXzKM2TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --quiet requests beautifulsoup4 pandas sqlalchemy pymysql\n",
        "!wget -q -O exchange_rate.csv \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0221EN-Coursera/labs/v2/exchange_rate.csv\"\n",
        "!ls -l exchange_rate.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQj0EycVObe9",
        "outputId": "91ad5fb3-3de4-41e0-da8b-470cb001fd8b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 45 Sep  8  2023 exchange_rate.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preliminaries:** We use requests, BeautifulSoup (bs4), pandas, numpy, sqlite3 and datetime."
      ],
      "metadata": {
        "id": "HtEHrmjfOxbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and project constants\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Project constants\n",
        "ARCHIVE_BANKS_URL = \"https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks\"\n",
        "EXCHANGE_RATE_CSV = \"exchange_rate.csv\"\n",
        "OUTPUT_CSV = \"Largest_banks_data.csv\"\n",
        "SQLITE_DB = \"Banks.db\"\n",
        "TABLE_NAME = \"Largest_banks\"\n",
        "LOG_FILE = \"code_log.txt\"\n",
        "\n",
        "LOG_MESSAGES = {\n",
        "    \"init\": \"Preliminaries complete. Initiating ETL process\",\n",
        "    \"extracted\": \"Data extraction complete. Initiating Transformation process\",\n",
        "    \"transformed\": \"Data transformation complete. Initiating Loading process\",\n",
        "    \"csv_saved\": \"Data saved to CSV file\",\n",
        "    \"sql_conn\": \"SQL Connection initiated\",\n",
        "    \"db_loaded\": \"Data loaded to Database as a table, Executing queries\",\n",
        "    \"process_complete\": \"Process Complete\",\n",
        "    \"sql_closed\": \"Server Connection closed\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "ptREdFXBPNw5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 — Logging**  \n",
        "Format: `<time_stamp> : <message>` (each entry on a new line). We'll call this function at required stages."
      ],
      "metadata": {
        "id": "MivUBBMFPc75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_progress(message, log_file=LOG_FILE):\n",
        "    \"\"\"Append a timestamped message to the log file using format: <time_stamp> : <message>\"\"\"\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    entry = f\"{ts} : {message}\\n\"\n",
        "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(entry)\n"
      ],
      "metadata": {
        "id": "_RSbslEKPq-E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 — Extraction**  \n",
        "Goal: find table under heading \"By market capitalization\", extract Name and Market Cap (in USD, billions), clean values (remove trailing characters, commas, footnotes), convert to float, keep top 10 by MC_USD_Billion.\n"
      ],
      "metadata": {
        "id": "GTGXaYiBPzDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(url=ARCHIVE_BANKS_URL):\n",
        "    \"\"\"Extract Name and MC_USD_Billion from the archived Wikipedia page and return top-10 dataframe.\"\"\"\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # Locate the 'By market capitalization' heading then next table\n",
        "    header = soup.find(lambda tag: tag.name in [\"h2\", \"h3\"] and \"By market capitalization\" in tag.get_text())\n",
        "    if header:\n",
        "        table = header.find_next(\"table\")\n",
        "    else:\n",
        "        # fallback: pick first table that looks like market caps\n",
        "        tables = soup.find_all(\"table\")\n",
        "        table = None\n",
        "        for t in tables:\n",
        "            text = t.get_text()\n",
        "            if \"Market cap\" in text or \"Market capitalization\" in text or \"Market\" in text:\n",
        "                table = t\n",
        "                break\n",
        "        if table is None:\n",
        "            raise ValueError(\"Required table not found on page\")\n",
        "\n",
        "    # Parse table with pandas\n",
        "    df_raw = pd.read_html(str(table))[0]\n",
        "    df_raw.columns = [str(c).strip() for c in df_raw.columns]\n",
        "\n",
        "    # Heuristics to pick name and market cap columns\n",
        "    name_col = None\n",
        "    mc_col = None\n",
        "    for c in df_raw.columns:\n",
        "        low = c.lower()\n",
        "        if name_col is None and (\"name\" in low or \"bank\" in low):\n",
        "            name_col = c\n",
        "        if mc_col is None and (\"market\" in low or \"market cap\" in low or \"mc\" in low):\n",
        "            mc_col = c\n",
        "\n",
        "    if name_col is None:\n",
        "        name_col = df_raw.columns[0]\n",
        "    if mc_col is None:\n",
        "        mc_col = df_raw.columns[-1]\n",
        "\n",
        "    df = df_raw[[name_col, mc_col]].copy()\n",
        "    df.columns = [\"Name\", \"MC_USD_Billion\"]\n",
        "\n",
        "    # cleaning function: remove commas, footnotes and extract first numeric token\n",
        "    import re\n",
        "    def clean_mc(x):\n",
        "        if pd.isna(x):\n",
        "            return np.nan\n",
        "        s = str(x).split(\"[\")[0]   # remove bracketed footnotes\n",
        "        s = s.replace(\",\", \"\").replace(\"billion\", \"\").strip()\n",
        "        m = re.search(r\"[-+]?[0-9]*\\.?[0-9]+\", s)\n",
        "        return float(m.group(0)) if m else np.nan\n",
        "\n",
        "    df[\"MC_USD_Billion\"] = df[\"MC_USD_Billion\"].apply(clean_mc)\n",
        "    df = df.dropna(subset=[\"MC_USD_Billion\"]).reset_index(drop=True)\n",
        "    df = df.sort_values(\"MC_USD_Billion\", ascending=False).head(10).reset_index(drop=True)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "wF9PdMr3P9Zt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWUD-KemMnAp",
        "outputId": "c84445d8-77a0-45a9-c39a-b4845d5a5d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      Name  MC_USD_Billion\n",
            "0                           JPMorgan Chase          432.92\n",
            "1                          Bank of America          231.52\n",
            "2  Industrial and Commercial Bank of China          194.56\n",
            "3               Agricultural Bank of China          160.68\n",
            "4                                HDFC Bank          157.91\n",
            "5                              Wells Fargo          155.87\n",
            "6                        HSBC Holdings PLC          148.90\n",
            "7                           Morgan Stanley          140.83\n",
            "8                  China Construction Bank          139.82\n",
            "9                            Bank of China          136.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1392621929.py:24: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  df_raw = pd.read_html(str(table))[0]\n"
          ]
        }
      ],
      "source": [
        "# Make first log entry (Preliminaries complete)\n",
        "if os.path.exists(LOG_FILE):\n",
        "    pass\n",
        "log_progress(LOG_MESSAGES[\"init\"])\n",
        "\n",
        "# Extract and preview\n",
        "df_extracted = extract()\n",
        "print(df_extracted)\n",
        "log_progress(LOG_MESSAGES[\"extracted\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 — Transformation**  \n",
        "Read `exchange_rate.csv` and convert to a dict. Add `MC_GBP_Billion`, `MC_EUR_Billion`, `MC_INR_Billion` = USD * rate (rounded to 2 decimals). Then return final df with columns: Name, MC_USD_Billion, MC_GBP_Billion, MC_EUR_Billion, MC_INR_Billion."
      ],
      "metadata": {
        "id": "Kxrc02NxQnBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(df, csv_path=EXCHANGE_RATE_CSV):\n",
        "    \"\"\"Add MC_GBP_Billion, MC_EUR_Billion, MC_INR_Billion to df using exchange_rate.csv\"\"\"\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"{csv_path} not found. Download it and re-run the cell.\")\n",
        "\n",
        "    # Try reading with or without header\n",
        "    ex = pd.read_csv(csv_path, header=None)\n",
        "    # If file has header row with strings, try again with headers\n",
        "    if ex.shape[1] >= 2 and ex.iloc[0].dtype == object:\n",
        "        try:\n",
        "            ex = pd.read_csv(csv_path)\n",
        "        except Exception:\n",
        "            ex = pd.read_csv(csv_path, header=None)\n",
        "\n",
        "    keys = ex.iloc[:, 0].astype(str).str.strip().tolist()\n",
        "    vals = pd.to_numeric(ex.iloc[:, 1], errors=\"coerce\").tolist()\n",
        "    exchange_rate = dict(zip(keys, vals))\n",
        "\n",
        "    # required currencies\n",
        "    for c in [\"GBP\", \"EUR\", \"INR\"]:\n",
        "        if c not in exchange_rate:\n",
        "            raise KeyError(f\"Exchange rate for {c} not found in {csv_path}\")\n",
        "\n",
        "    df2 = df.copy()\n",
        "    df2[\"MC_GBP_Billion\"] = [np.round(x * exchange_rate[\"GBP\"], 2) for x in df2[\"MC_USD_Billion\"]]\n",
        "    df2[\"MC_EUR_Billion\"] = [np.round(x * exchange_rate[\"EUR\"], 2) for x in df2[\"MC_USD_Billion\"]]\n",
        "    df2[\"MC_INR_Billion\"] = [np.round(x * exchange_rate[\"INR\"], 2) for x in df2[\"MC_USD_Billion\"]]\n",
        "\n",
        "    # reorder final columns as required\n",
        "    df2 = df2[[\"Name\", \"MC_USD_Billion\", \"MC_GBP_Billion\", \"MC_EUR_Billion\", \"MC_INR_Billion\"]]\n",
        "    return df2"
      ],
      "metadata": {
        "id": "vFNb0nwLQ5hW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed = transform(df_extracted, EXCHANGE_RATE_CSV)\n",
        "print(df_transformed)\n",
        "log_progress(LOG_MESSAGES[\"transformed\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27mvS_rMRL9t",
        "outputId": "5909bb44-43f2-4dcc-f6e8-6161e945e272"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      Name  MC_USD_Billion  MC_GBP_Billion  \\\n",
            "0                           JPMorgan Chase          432.92          346.34   \n",
            "1                          Bank of America          231.52          185.22   \n",
            "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
            "3               Agricultural Bank of China          160.68          128.54   \n",
            "4                                HDFC Bank          157.91          126.33   \n",
            "5                              Wells Fargo          155.87          124.70   \n",
            "6                        HSBC Holdings PLC          148.90          119.12   \n",
            "7                           Morgan Stanley          140.83          112.66   \n",
            "8                  China Construction Bank          139.82          111.86   \n",
            "9                            Bank of China          136.81          109.45   \n",
            "\n",
            "   MC_EUR_Billion  MC_INR_Billion  \n",
            "0          402.62        35910.71  \n",
            "1          215.31        19204.58  \n",
            "2          180.94        16138.75  \n",
            "3          149.43        13328.41  \n",
            "4          146.86        13098.63  \n",
            "5          144.96        12929.42  \n",
            "6          138.48        12351.26  \n",
            "7          130.97        11681.85  \n",
            "8          130.03        11598.07  \n",
            "9          127.23        11348.39  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4 — Save CSV**  \n",
        "Save final DataFrame to `./Largest_banks_data.csv` and log the completion message.\n"
      ],
      "metadata": {
        "id": "CENQgeRxRf23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_to_csv(df, output_path=OUTPUT_CSV):\n",
        "    df.to_csv(output_path, index=False)\n",
        "    log_progress(LOG_MESSAGES[\"csv_saved\"])\n",
        "\n",
        "# save CSV and preview first lines\n",
        "load_to_csv(df_transformed, OUTPUT_CSV)\n",
        "!head -n 12 Largest_banks_data.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MM_Z4GnRlSf",
        "outputId": "d4f161ec-11fb-4611-9c4c-50f939f1e5c6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name,MC_USD_Billion,MC_GBP_Billion,MC_EUR_Billion,MC_INR_Billion\n",
            "JPMorgan Chase,432.92,346.34,402.62,35910.71\n",
            "Bank of America,231.52,185.22,215.31,19204.58\n",
            "Industrial and Commercial Bank of China,194.56,155.65,180.94,16138.75\n",
            "Agricultural Bank of China,160.68,128.54,149.43,13328.41\n",
            "HDFC Bank,157.91,126.33,146.86,13098.63\n",
            "Wells Fargo,155.87,124.7,144.96,12929.42\n",
            "HSBC Holdings PLC,148.9,119.12,138.48,12351.26\n",
            "Morgan Stanley,140.83,112.66,130.97,11681.85\n",
            "China Construction Bank,139.82,111.86,130.03,11598.07\n",
            "Bank of China,136.81,109.45,127.23,11348.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5 — Save to DB**  \n",
        "We will use SQLite (`Banks.db`). Create connection, log SQL connection initiated, load table `Largest_banks`.\n"
      ],
      "metadata": {
        "id": "mJ9fu_Q2SEv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_to_db(df, sql_connection, table_name=TABLE_NAME):\n",
        "    \"\"\"Load dataframe into given sqlite3.Connection (or raise if not sqlite).\"\"\"\n",
        "    if isinstance(sql_connection, sqlite3.Connection):\n",
        "        df.to_sql(table_name, sql_connection, if_exists=\"replace\", index=False)\n",
        "    else:\n",
        "        raise ValueError(\"This Colab flow expects a sqlite3.Connection.\")\n",
        "    log_progress(LOG_MESSAGES[\"db_loaded\"])\n",
        "\n",
        "# Connect to SQLite and load\n",
        "conn = sqlite3.connect(SQLITE_DB)\n",
        "log_progress(LOG_MESSAGES[\"sql_conn\"])\n",
        "load_to_db(df_transformed, conn, TABLE_NAME)\n"
      ],
      "metadata": {
        "id": "jh_9l9LOSdum"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6 — Run queries**  \n",
        "Run and print outputs for:\n",
        "1. `SELECT * FROM Largest_banks`  \n",
        "2. `SELECT AVG(MC_GBP_Billion) FROM Largest_banks`  \n",
        "3. `SELECT Name FROM Largest_banks LIMIT 5`\n"
      ],
      "metadata": {
        "id": "dqkyzpuDSk09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query(query_statement, sql_connection):\n",
        "    \"\"\"Execute query on sqlite3.Connection and print results.\"\"\"\n",
        "    print(\"Query:\", query_statement)\n",
        "    df_out = pd.read_sql_query(query_statement, sql_connection)\n",
        "    print(df_out)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Execute required queries\n",
        "run_query(f\"SELECT * FROM {TABLE_NAME}\", conn)\n",
        "run_query(f\"SELECT AVG(MC_GBP_Billion) FROM {TABLE_NAME}\", conn)\n",
        "run_query(f\"SELECT Name FROM {TABLE_NAME} LIMIT 5\", conn)\n",
        "\n",
        "log_progress(LOG_MESSAGES[\"process_complete\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMMr9lqYSsRX",
        "outputId": "571920c2-1f29-4fae-f5f7-81d518690a30"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: SELECT * FROM Largest_banks\n",
            "                                      Name  MC_USD_Billion  MC_GBP_Billion  \\\n",
            "0                           JPMorgan Chase          432.92          346.34   \n",
            "1                          Bank of America          231.52          185.22   \n",
            "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
            "3               Agricultural Bank of China          160.68          128.54   \n",
            "4                                HDFC Bank          157.91          126.33   \n",
            "5                              Wells Fargo          155.87          124.70   \n",
            "6                        HSBC Holdings PLC          148.90          119.12   \n",
            "7                           Morgan Stanley          140.83          112.66   \n",
            "8                  China Construction Bank          139.82          111.86   \n",
            "9                            Bank of China          136.81          109.45   \n",
            "\n",
            "   MC_EUR_Billion  MC_INR_Billion  \n",
            "0          402.62        35910.71  \n",
            "1          215.31        19204.58  \n",
            "2          180.94        16138.75  \n",
            "3          149.43        13328.41  \n",
            "4          146.86        13098.63  \n",
            "5          144.96        12929.42  \n",
            "6          138.48        12351.26  \n",
            "7          130.97        11681.85  \n",
            "8          130.03        11598.07  \n",
            "9          127.23        11348.39  \n",
            "----------------------------------------\n",
            "Query: SELECT AVG(MC_GBP_Billion) FROM Largest_banks\n",
            "   AVG(MC_GBP_Billion)\n",
            "0              151.987\n",
            "----------------------------------------\n",
            "Query: SELECT Name FROM Largest_banks LIMIT 5\n",
            "                                      Name\n",
            "0                           JPMorgan Chase\n",
            "1                          Bank of America\n",
            "2  Industrial and Commercial Bank of China\n",
            "3               Agricultural Bank of China\n",
            "4                                HDFC Bank\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conn.close()\n",
        "log_progress(LOG_MESSAGES[\"sql_closed\"])\n",
        "\n",
        "# show produced files for verification\n",
        "!ls -l | grep -E \"Largest_banks_data.csv|Banks.db|code_log.txt\" || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A9-6rUWTofg",
        "outputId": "b749a374-34eb-4cd5-aab3-fdf8a6a17b0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 8192 Dec 11 07:22 Banks.db\n",
            "-rw-r--r-- 1 root root  531 Dec 11 07:28 code_log.txt\n",
            "-rw-r--r-- 1 root root  554 Dec 11 07:18 Largest_banks_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the log file content for Task 7 verification\n",
        "print(\"---- code_log.txt ----\")\n",
        "if os.path.exists(LOG_FILE):\n",
        "    with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"code_log.txt not found. If you re-run notebook after removing previous logs, the file will be created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcLK8hgBUdSp",
        "outputId": "c548164a-c4bb-4e45-c484-f2ea4f1344ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- code_log.txt ----\n",
            "2025-12-11 07:35:01 : Preliminaries complete. Initiating ETL process\n",
            "2025-12-11 07:35:01 : Data extraction complete. Initiating Transformation process\n",
            "2025-12-11 07:35:13 : Data transformation complete. Initiating Loading process\n",
            "2025-12-11 07:35:17 : Data saved to CSV file\n",
            "2025-12-11 07:35:20 : SQL Connection initiated\n",
            "2025-12-11 07:35:20 : Data loaded to Database as a table, Executing queries\n",
            "2025-12-11 07:35:25 : Process Complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This ETL pipeline demonstrates my ability to perform real-world data engineering tasks, including web scraping, data cleaning, transformation using external data sources, structured logging, and loading results into both CSV and SQL databases. This project is completed as part of the IBM Data Engineering Specialization and is included in my portfolio to showcase practical ETL development skills.\n"
      ],
      "metadata": {
        "id": "DtoHY0j_V2o6"
      }
    }
  ]
}