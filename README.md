ETL Pipeline: Worldâ€™s Largest Banks Data Processing
IBM Data Engineering Specialization â€“ Portfolio Project

This project demonstrates a complete Extract, Transform, Load (ETL) workflow using Python, web scraping, data cleaning, currency conversion, structured logging, and database loading.
The goal is to extract the top 10 largest banks in the world by market capitalization (USD), transform the values into GBP, EUR, and INR, and load the processed data into CSV and a SQLite database.

ğŸ“Œ Project Objectives

Extract market capitalization data for the worldâ€™s top banks from an archived web source.

Clean and structure the raw data into a pandas DataFrame.

Convert market capitalization from USD â†’ GBP, EUR, INR using exchange_rate.csv.

Load the final dataset into:

A CSV file

A SQLite database (Banks.db)

Maintain execution history using a detailed code_log.txt.

Present the project as a real-world portfolio ETL pipeline.

ğŸ›  Tech Stack

Python

BeautifulSoup (bs4) â€” Web scraping

Requests â€” Data extraction

Pandas / NumPy â€” Data transformation

SQLite3 â€” Database creation & loading

Google Colab â€” Development environment

Markdown + GitHub â€” Documentation & portfolio presentation

ğŸ“‚ Repository Structure
world-top-banks-etl/
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ exchange_rate.csv
â”‚   â”œâ”€â”€ Largest_banks_data.csv
â”‚   â””â”€â”€ Banks.db
â”‚
â”œâ”€â”€ Notebook/
â”‚   â””â”€â”€ world_top_banks.ipynb
â”‚
â”œâ”€â”€ Results/
â”‚   â”œâ”€â”€ extract.png
â”‚   â”œâ”€â”€ transform.png
â”‚   â”œâ”€â”€ save_csv.png
â”‚   â”œâ”€â”€ queries_results.png
â”‚   â”œâ”€â”€ files_log.png
â”‚   â””â”€â”€ etl_process_log.png
â”‚
â”œâ”€â”€ code_log.txt
â””â”€â”€ README.md

ğŸ”„ ETL Workflow Overview
1ï¸âƒ£ Extract

Scrape table under "By market capitalization" from the archived Wikipedia page.

Clean text, remove footnotes, parse numbers, and select top 10 rows.

2ï¸âƒ£ Transform

Load exchange rate CSV

Convert MC_USD_Billion to:

MC_GBP_Billion

MC_EUR_Billion

MC_INR_Billion

Round values to 2 decimal places

Reorder final columns

3ï¸âƒ£ Load

Save transformed dataset to Largest_banks_data.csv

Load final table into Banks.db under table name Largest_banks

4ï¸âƒ£ Logging

Each major step writes an entry to code_log.txt in the format:

YYYY-MM-DD HH:MM:SS : <status message>

â–¶ï¸ How to Run the Project

Open the notebook:

Notebook/world_top_banks.ipynb


Run all cells top to bottom in Google Colab.

Make sure exchange_rate.csv is located inside the Data folder.

Outputs will appear in the Data folder and logs in the repo root.

ğŸ“¸ Results & Screenshots

Screenshots of extraction, transformation, saving, SQL outputs, and logs are available in the Results/ directory.

These help demonstrate work for peer review and portfolio presentation.

ğŸ“˜ Key Skills Demonstrated

Web scraping using BeautifulSoup

ETL Pipeline development

Data cleaning & transformation

Working with external CSV sources

SQL database creation & loading

Structured logging for ETL processes

Organizing a project for real-world portfolios

Git & GitHub best practices

ğŸ¯ About This Project

This project was completed as part of the IBM Data Engineering Specialization and uploaded to GitHub as a portfolio project to demonstrate practical ETL development capability using Python and SQL.

â­ If you like this project

Feel free to star â­ the repo or connect with me on LinkedIn!
